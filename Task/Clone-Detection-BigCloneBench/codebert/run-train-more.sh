#!/bin/bash
set -euo pipefail

# ================================================================
# run-train-more-codebert.sh
# Fine-tune CodeBERT on mixed dataset: BCB(10% balanced) + Other Domain (e.g., Camel)
#
# Inputs:
#   ../dataset/train_mix.txt
#   ../dataset/valid_mix.txt
#   ../dataset/test_<otherdomain>.txt   (OTHER-domain-only test, e.g., test_camel.txt)
#   ../dataset/mix/data.jsonl           (loaded via --test_type mix)
#
# Key args:
#   --test_type mix        -> makes run.py load ../dataset/mix/data.jsonl
#   --subsample_ratio 1.0  -> use full mixed data (do NOT subsample again)
#
# Notes:
#   - Train/Valid are MIXED (BCB + other domain).
#   - Test is OTHER-DOMAIN ONLY to measure cross-domain generalization.
# ================================================================

echo ">>> [System Config] Detecting Hardware..."

NUM_GPUS=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)

# Hyperparameters (tunable)
PER_GPU_BATCH=8
TARGET_GLOBAL_BATCH=64
EPOCHS=2

ACCUM_STEPS=$(( TARGET_GLOBAL_BATCH / (PER_GPU_BATCH * NUM_GPUS) ))
[ "$ACCUM_STEPS" -lt 1 ] && ACCUM_STEPS=1

echo "    - GPUs Detected : $NUM_GPUS"
echo "    - Per-GPU Batch : $PER_GPU_BATCH"
echo "    - Accumulation  : $ACCUM_STEPS"
echo "    - Global Batch  : $TARGET_GLOBAL_BATCH"
echo "    - Epochs        : $EPOCHS"

LAUNCH_CMD="accelerate launch"
[ "$NUM_GPUS" -gt 1 ] && LAUNCH_CMD="$LAUNCH_CMD --multi_gpu --num_processes $NUM_GPUS"

OUTDIR="./saved_models_codebert_mix"
mkdir -p "$OUTDIR"

# Paper setting
TOKENIZER=microsoft/codebert-base   # solution default
# TOKENIZER="roberta-base"

# Other-domain test file (generated by 2_mix_data.py)
TEST_FILE="../dataset/test_camel.txt"
[ -f "$TEST_FILE" ] || { echo "ERROR: Missing test file: $TEST_FILE"; exit 1; }

echo ">>> [Start] Fine-tuning CodeBERT on mixed dataset (BCB10% + Other Domain)"
echo "    - Output Dir : $OUTDIR"
echo "    - Log File   : $OUTDIR/train_mix.log"
echo "    - Train/Valid: ../dataset/train_mix.txt / ../dataset/valid_mix.txt"
echo "    - Test (OD)  : $TEST_FILE"
echo "    - Mapping    : ../dataset/mix/data.jsonl (via --test_type mix)"
echo "    - Base Model : microsoft/codebert-base"
echo "    - Tokenizer  : $TOKENIZER"

$LAUNCH_CMD run.py \
  --output_dir="$OUTDIR/" \
  --model_type=roberta \
  --config_name=microsoft/codebert-base \
  --model_name_or_path=microsoft/codebert-base \
  --tokenizer_name="$TOKENIZER" \
  --test_type mix \
  --do_train \
  --do_test \
  --train_data_file=../dataset/train_mix.txt \
  --eval_data_file=../dataset/valid_mix.txt \
  --test_data_file="$TEST_FILE" \
  --block_size 400 \
  --train_batch_size "$PER_GPU_BATCH" \
  --gradient_accumulation_steps "$ACCUM_STEPS" \
  --eval_batch_size 32 \
  --epoch "$EPOCHS" \
  --learning_rate 5e-5 \
  --max_grad_norm 1.0 \
  --evaluate_during_training \
  --save_steps 500 \
  --logging_steps 100 \
  --save_total_limit 2 \
  --seed 3 2>&1 | tee "$OUTDIR/train_mix.log"
